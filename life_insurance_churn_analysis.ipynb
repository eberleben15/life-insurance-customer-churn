{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2690465a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display configuration set to show full output without truncation.\n",
      "Helper functions added: show_full_dataframe(), show_model_summary(), analyze_feature()\n"
     ]
    }
   ],
   "source": [
    "# Configuration to display all output without truncation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set pandas display options to show all rows and columns\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # Auto-detect display width\n",
    "pd.set_option('display.max_colwidth', None)  # Show full content of each column\n",
    "pd.set_option('display.precision', 2)  # Round floating point numbers to 2 decimal places\n",
    "\n",
    "# Set numpy print options\n",
    "np.set_printoptions(threshold=np.inf)  # Print full numpy arrays\n",
    "np.set_printoptions(precision=2)  # Round numpy values to 2 decimal places\n",
    "np.set_printoptions(suppress=True)  # Suppress scientific notation\n",
    "\n",
    "# Make plots larger and more readable\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 8]  # Default figure size\n",
    "plt.rcParams['font.size'] = 12  # Larger font size\n",
    "\n",
    "# Helper function to display full dataframes with styling\n",
    "def show_full_dataframe(df, title=None, max_rows=None):\n",
    "    \"\"\"Display a full dataframe with styling and an optional title\"\"\"\n",
    "    if title:\n",
    "        display(HTML(f\"<h3>{title}</h3>\"))\n",
    "    \n",
    "    # Style the dataframe for better readability\n",
    "    styled_df = df.head(max_rows) if max_rows else df\n",
    "    styled_df = styled_df.style.set_properties(**{\n",
    "        'text-align': 'left',\n",
    "        'background-color': '#f5f5f5',\n",
    "        'border-color': 'white',\n",
    "        'border-style': 'solid',\n",
    "        'border-width': '1px'\n",
    "    }).set_table_styles([\n",
    "        {'selector': 'th', 'props': [('background-color', '#4CAF50'), \n",
    "                                    ('color', 'white'),\n",
    "                                    ('font-weight', 'bold'),\n",
    "                                    ('text-align', 'left')]},\n",
    "        {'selector': 'tr:nth-of-type(even)', 'props': [('background-color', '#f2f2f2')]},\n",
    "    ])\n",
    "    \n",
    "    display(styled_df)\n",
    "    if max_rows and len(df) > max_rows:\n",
    "        print(f\"Showing {max_rows} of {len(df)} rows. Full data has shape: {df.shape}\")\n",
    "\n",
    "# Helper function to show model summary with formatting\n",
    "def show_model_summary(model):\n",
    "    \"\"\"Display a TensorFlow model summary in a formatted way\"\"\"\n",
    "    # Capture the model summary\n",
    "    from io import StringIO\n",
    "    import sys\n",
    "    \n",
    "    original_stdout = sys.stdout\n",
    "    string_io = StringIO()\n",
    "    sys.stdout = string_io\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    sys.stdout = original_stdout\n",
    "    summary_string = string_io.getvalue()\n",
    "    \n",
    "    # Display with HTML formatting\n",
    "    formatted_summary = summary_string.replace('\\n', '<br>')\n",
    "    display(HTML(f\"<pre style='background-color: #f0f0f0; padding: 10px; border-radius: 5px;'>{formatted_summary}</pre>\"))\n",
    "\n",
    "# Helper function for detailed feature analysis\n",
    "def analyze_feature(df, feature_name, target='Churn'):\n",
    "    \"\"\"Perform detailed analysis of a single feature\"\"\"\n",
    "    print(f\"\\n===== Detailed Analysis of '{feature_name}' =====\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    stats = df[feature_name].describe()\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(stats)\n",
    "    \n",
    "    # Relationship with target\n",
    "    if df[feature_name].dtype in ['int64', 'float64']:\n",
    "        # For numeric features\n",
    "        print(\"\\nDistribution by Target:\")\n",
    "        grouped = df.groupby(target)[feature_name].agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "        print(grouped)\n",
    "        \n",
    "        # Plot distribution\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        for target_val in sorted(df[target].unique()):\n",
    "            subset = df[df[target] == target_val]\n",
    "            plt.hist(subset[feature_name], alpha=0.5, bins=30, \n",
    "                     label=f\"{target}={target_val}\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Distribution of {feature_name} by {target}\")\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.boxplot([df[df[target]==t][feature_name] for t in sorted(df[target].unique())], \n",
    "                   labels=[f\"{target}={t}\" for t in sorted(df[target].unique())])\n",
    "        plt.title(f\"Boxplot of {feature_name} by {target}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        # For categorical features\n",
    "        print(\"\\nFrequency by Target:\")\n",
    "        crosstab = pd.crosstab(df[feature_name], df[target], normalize='index') * 100\n",
    "        crosstab.columns = [f\"{target}={c}\" for c in crosstab.columns]\n",
    "        print(crosstab)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        crosstab.plot(kind='bar', stacked=True)\n",
    "        plt.title(f\"{feature_name} vs {target}\")\n",
    "        plt.ylabel(\"Percentage\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"Display configuration set to show full output without truncation.\")\n",
    "print(\"Helper functions added: show_full_dataframe(), show_model_summary(), analyze_feature()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c57a771f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msys\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Environment Information\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"seaborn version: {sns.__version__}\")\n",
    "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Notebook last updated: June 24, 2025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a10e93",
   "metadata": {},
   "source": [
    "# Life Insurance Customer Churn Prediction with Deep Learning\n",
    "\n",
    "## Project Overview\n",
    "This project applies deep learning techniques to analyze and predict customer churn in a life insurance dataset. By leveraging neural networks, we aim to identify patterns and factors that contribute to customers discontinuing their insurance policies. The project demonstrates the effectiveness of deep learning compared to traditional machine learning approaches for this classification task.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Preparation](#data-preparation)\n",
    "2. [Exploratory Data Analysis](#exploratory-data-analysis)\n",
    "3. [Feature Engineering](#feature-engineering)\n",
    "4. [Neural Network Modeling](#neural-network-modeling)\n",
    "5. [Model Evaluation](#model-evaluation)\n",
    "6. [Conclusions and Future Work](#conclusions)\n",
    "\n",
    "## Project Objectives\n",
    "- Apply deep learning techniques to solve a binary classification problem\n",
    "- Compare the performance of neural networks with baseline models\n",
    "- Implement and evaluate regularization techniques (dropout, batch normalization)\n",
    "- Analyze feature importance using permutation importance and visualization\n",
    "- Explore how deep learning can capture complex non-linear relationships in data\n",
    "- Practice proper model evaluation and interpretation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b527e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Set visualization style for academic presentation\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('colorblind')  # Colorblind-friendly palette\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Load data\n",
    "print('Loading and preparing data...')\n",
    "df = pd.read_csv('data/life_insurance_churn.csv')\n",
    "\n",
    "# Check for and drop the 'Unnamed: 0' column (which is likely just an index from previous export)\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    print(f\"Dropping 'Unnamed: 0' column (previous index)\")\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Display initial column names\n",
    "print(f\"Columns in the dataset: {df.columns.tolist()}\")\n",
    "\n",
    "# Basic data cleaning\n",
    "df.columns = df.columns.str.replace(' ', '_').str.replace('/', '_')  # Clean column names\n",
    "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})  # Convert target to binary\n",
    "df = df.dropna(subset=['Churn'])  # Remove rows with missing target\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Churn rate: {df['Churn'].mean():.2%}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Using GPU: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa608016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Assessment\n",
    "print('Columns:', df.columns.tolist())\n",
    "\n",
    "print('\\nMissing values per column:')\n",
    "missing_data = df.isnull().sum().to_frame()\n",
    "missing_data.columns = ['Missing Values']\n",
    "missing_data['Percentage'] = (df.isnull().sum() / len(df) * 100).values\n",
    "# Only show columns with missing values\n",
    "missing_data = missing_data[missing_data['Missing Values'] > 0]\n",
    "if len(missing_data) > 0:\n",
    "    show_full_dataframe(missing_data, \"Columns with Missing Values\")\n",
    "else:\n",
    "    print(\"No missing values found in the dataset\")\n",
    "\n",
    "print('\\nBasic statistics:')\n",
    "show_full_dataframe(df.describe().T, \"Statistical Summary of Numeric Features\")\n",
    "\n",
    "# Enhanced data type and value count information\n",
    "print('\\nData types and unique values:')\n",
    "data_types = pd.DataFrame({\n",
    "    'Data Type': df.dtypes,\n",
    "    'Unique Values': df.nunique(),\n",
    "    'Sample Values': [str(df[col].dropna().sample(3).tolist())[:50] + '...' \n",
    "                      if df[col].nunique() > 3 else df[col].unique() \n",
    "                      for col in df.columns]\n",
    "})\n",
    "show_full_dataframe(data_types, \"Data Types and Value Counts\")\n",
    "\n",
    "# Simple feature engineering for deep learning\n",
    "try:\n",
    "    # Ensure numeric columns are properly converted\n",
    "    for col in ['Claim_Amount', 'Category_Premium', 'Premium_Amount_Ratio', 'BMI']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            print(f'Converted {col} to numeric. Range: {df[col].min()} to {df[col].max()}')\n",
    "\n",
    "    # Only process Company_Name if it exists and is not already one-hot encoded\n",
    "    if 'Company_Name' in df.columns:\n",
    "        n_companies = df['Company_Name'].nunique()\n",
    "        top_n = min(10, n_companies)\n",
    "        top_companies = df['Company_Name'].value_counts().nlargest(top_n).index\n",
    "        df['Company_Name'] = df['Company_Name'].where(df['Company_Name'].isin(top_companies), 'Other')\n",
    "        print(f'Processed Company_Name: {top_n} top companies retained, others grouped as \"Other\"')\n",
    "        \n",
    "        # Show the distribution of companies\n",
    "        company_counts = df['Company_Name'].value_counts()\n",
    "        print(\"\\nCompany distribution after processing:\")\n",
    "        show_full_dataframe(company_counts.to_frame(), \"Company Counts\")\n",
    "\n",
    "    # Simple ratio feature\n",
    "    if 'Claim_Amount' in df.columns and 'Category_Premium' in df.columns:\n",
    "        df['Claim_to_Premium_Ratio'] = np.where(\n",
    "            df['Category_Premium'] > 0,\n",
    "            df['Claim_Amount'] / df['Category_Premium'],\n",
    "            0\n",
    "        )\n",
    "        print('Created Claim_to_Premium_Ratio feature')\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    cat_cols = [col for col in ['Claim_Reason', 'Data_confidentiality', 'Claim_Request_output', 'Company_Name'] \n",
    "                if col in df.columns]\n",
    "    \n",
    "    if cat_cols:\n",
    "        for cat_col in cat_cols:\n",
    "            df[cat_col] = df[cat_col].fillna('Unknown')\n",
    "            print(f'Categories in {cat_col}: {df[cat_col].nunique()} unique values')\n",
    "            \n",
    "            # Show value counts for each categorical feature\n",
    "            value_counts = df[cat_col].value_counts(normalize=True).to_frame() * 100\n",
    "            value_counts.columns = ['Percentage']\n",
    "            print(f\"\\nDistribution of {cat_col}:\")\n",
    "            show_full_dataframe(value_counts, f\"{cat_col} Distribution (%)\")\n",
    "        \n",
    "        print('One-hot encoding categorical features...')\n",
    "        df = pd.get_dummies(df, columns=cat_cols, drop_first=False)\n",
    "        print(f'After encoding, dataframe has {df.shape[1]} columns')\n",
    "    else:\n",
    "        print('No categorical columns to encode or already encoded.')\n",
    "\n",
    "    # Prepare features and target\n",
    "    X = df.drop(['Churn', 'Customer_Name', 'Customer_Address'], axis=1, errors='ignore')\n",
    "    y = df['Churn']\n",
    "\n",
    "    # Remove columns with all NaN or inf\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).dropna(axis=1, how='all')\n",
    "\n",
    "    # Impute numeric NaNs with median\n",
    "    for col in X.select_dtypes(include=[np.number]).columns:\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "    # Fill any remaining NaNs (e.g., in dummies) with 0\n",
    "    X = X.fillna(0)\n",
    "\n",
    "    # Print feature summary before modeling\n",
    "    print('\\nFeature preparation complete:')\n",
    "    print(f'- Features: {X.shape[1]} columns')\n",
    "    print(f'- Samples: {X.shape[0]} rows')\n",
    "    print(f'- Target distribution:')\n",
    "    target_dist = y.value_counts(normalize=True).to_frame() * 100\n",
    "    target_dist.columns = ['Percentage']\n",
    "    show_full_dataframe(target_dist, \"Target Distribution (%)\")\n",
    "\n",
    "    # Train-test split with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Feature scaling - essential for deep learning\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(f'Data split into {X_train.shape[0]} training and {X_test.shape[0]} testing samples')\n",
    "    \n",
    "    # Show a sample of the training data to confirm structure\n",
    "    print(\"\\nSample of training features (first 5 rows, first 10 columns):\")\n",
    "    train_sample = pd.DataFrame(\n",
    "        X_train_scaled[:5, :10], \n",
    "        columns=X.columns[:10]\n",
    "    )\n",
    "    show_full_dataframe(train_sample, \"Sample of Scaled Training Data\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print('Feature engineering error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222eb206",
   "metadata": {},
   "source": [
    "# Data Preparation and Exploration <a id=\"data-preparation\"></a>\n",
    "\n",
    "## Dataset Overview\n",
    "- **Total Records:** The dataset contains information on insurance customers and their churn status\n",
    "- **Target Variable:** Churn (binary: Yes=1, No=0)\n",
    "- **Features:** The dataset includes demographic information, claim details, and premium information\n",
    "- **Missing Values:** Addressed through appropriate imputation strategies\n",
    "- **Categorical Features:** Encoded using one-hot encoding for neural network compatibility\n",
    "\n",
    "## Learning Objectives for Data Preparation\n",
    "1. **Data Cleaning**: Handling missing values and outliers appropriately\n",
    "2. **Data Transformation**: Converting categorical variables to numerical representations\n",
    "3. **Feature Engineering**: Creating meaningful features from existing data\n",
    "4. **Data Visualization**: Understanding the distribution and relationships between variables\n",
    "5. **Data Splitting**: Creating appropriate training and testing sets\n",
    "\n",
    "## Feature Categories\n",
    "1. **Customer Information:** Customer_Name, Customer_Address\n",
    "2. **Claim Information:** Claim_Reason, Claim_Amount, Claim_Request_output\n",
    "3. **Premium Information:** Category_Premium, Premium_Amount_Ratio\n",
    "4. **Company Information:** Company_Name\n",
    "5. **Other Attributes:** Data_confidentiality, BMI\n",
    "\n",
    "In the next sections, we'll visualize these features and analyze their relationships to customer churn, building a foundation for our neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize churn distribution\n",
    "sns.countplot(x='Churn', data=df, palette=['#4CAF50', '#F44336'])\n",
    "plt.title('Customer Churn Distribution')\n",
    "plt.xlabel('Churn')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['No', 'Yes'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad3292",
   "metadata": {},
   "source": [
    "**This plot shows how many customers stayed versus how many left (churned). It helps us see if churn is a common issue in this dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61894f3",
   "metadata": {},
   "source": [
    "## Deeper Exploratory Data Analysis (EDA)\n",
    "\n",
    "Understanding the drivers of churn is essential for effective business action. The following analysis explores how key features relate to customer churn, highlighting actionable trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af93d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn rate by Claim Reason\n",
    "if 'Claim_Reason' in df.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    churn_by_reason = df.groupby('Claim_Reason')['Churn'].mean().sort_values(ascending=False)\n",
    "    sns.barplot(x=churn_by_reason.index, y=churn_by_reason.values, palette='viridis')\n",
    "    plt.title('Churn Rate by Claim Reason')\n",
    "    plt.ylabel('Churn Rate')\n",
    "    plt.xlabel('Claim Reason')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Churn rate by Company Name (top 10)\n",
    "if 'Company_Name' in df.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    churn_by_company = df.groupby('Company_Name')['Churn'].mean().loc[top_companies]\n",
    "    sns.barplot(x=churn_by_company.index, y=churn_by_company.values, palette='magma')\n",
    "    plt.title('Churn Rate by Company (Top 10)')\n",
    "    plt.ylabel('Churn Rate')\n",
    "    plt.xlabel('Company Name')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Churn rate by Data Confidentiality\n",
    "if 'Data_confidentiality' in df.columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    churn_by_conf = df.groupby('Data_confidentiality')['Churn'].mean()\n",
    "    sns.barplot(x=churn_by_conf.index, y=churn_by_conf.values, palette='coolwarm')\n",
    "    plt.title('Churn Rate by Data Confidentiality')\n",
    "    plt.ylabel('Churn Rate')\n",
    "    plt.xlabel('Data Confidentiality')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228f469c",
   "metadata": {},
   "source": [
    "**These bar charts show the churn rate for different groups, such as claim reasons, companies, and data confidentiality levels. This helps us spot which groups are more likely to leave.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8109373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for key numeric features by churn status\n",
    "num_features = ['Claim_Amount', 'Category_Premium', 'Premium_Amount_Ratio', 'BMI', 'Claim_to_Premium_Ratio']\n",
    "for col in num_features:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(7, 3))\n",
    "        sns.kdeplot(data=df, x=col, hue='Churn', fill=True, common_norm=False, palette=['#4CAF50', '#F44336'])\n",
    "        plt.title(f'{col.replace(\"_\", \" \")} Distribution by Churn Status')\n",
    "        plt.xlabel(col.replace('_', ' '))\n",
    "        plt.ylabel('Density')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c572d39a",
   "metadata": {},
   "source": [
    "**These plots compare the distributions of key numeric features for customers who stayed versus those who churned. This helps us see if certain values are linked to higher churn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47060d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numeric features\n",
    "plt.figure(figsize=(8, 6))\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "corr = df[numeric_cols].corr()\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='Blues', square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Correlation Heatmap of Numeric Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b86d80",
   "metadata": {},
   "source": [
    "**This heatmap shows how numeric features are related to each other. Strong correlations can reveal which features move together and may affect churn.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfb8a62",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We create features based on available columns to improve model performance and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bbc61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for Deep Learning <a id=\"feature-engineering\"></a>\n",
    "\n",
    "print(\"\\n=== Feature Engineering and Deep Learning Preparation ===\\n\")\n",
    "\n",
    "# 1. Data Type Conversion and Standardization\n",
    "print(\"1. Converting and validating numeric features...\")\n",
    "for col in ['Claim_Amount', 'Category_Premium', 'Premium_Amount_Ratio', 'BMI']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        print(f\"   - {col}: Range [{df[col].min():.2f} to {df[col].max():.2f}], Missing: {df[col].isna().sum()}\")\n",
    "\n",
    "# 2. Company Name Processing - simplify categorical values\n",
    "print(\"\\n2. Processing categorical variables...\")\n",
    "if 'Company_Name' in df.columns:\n",
    "    n_companies = df['Company_Name'].nunique()\n",
    "    top_n = min(10, n_companies)\n",
    "    top_companies = df['Company_Name'].value_counts().nlargest(top_n).index\n",
    "    \n",
    "    # Group less common companies\n",
    "    df['Company_Name'] = df['Company_Name'].where(df['Company_Name'].isin(top_companies), 'Other')\n",
    "    print(f\"   - Reduced Company_Name from {n_companies} to {df['Company_Name'].nunique()} categories\")\n",
    "\n",
    "# 3. Create relevant features for deep learning\n",
    "print(\"\\n3. Creating features for neural network input...\")\n",
    "\n",
    "# Simple ratio features that capture important relationships\n",
    "df['Claim_to_Premium_Ratio'] = df['Claim_Amount'] / df['Category_Premium'].replace(0, 0.001)\n",
    "print(f\"   - Created Claim_to_Premium_Ratio feature\")\n",
    "\n",
    "# Log transform skewed numeric features to improve neural network performance\n",
    "skewed_features = ['Claim_Amount', 'Category_Premium']\n",
    "for col in skewed_features:\n",
    "    if col in df.columns:\n",
    "        # Add small constant to handle zeros before log transform\n",
    "        df[f'{col}_Log'] = np.log1p(df[col])\n",
    "        print(f\"   - Created log-transformed feature: {col}_Log\")\n",
    "\n",
    "# 4. Categorical Encoding\n",
    "print(\"\\n4. Encoding categorical features for neural networks...\")\n",
    "cat_cols = [col for col in ['Claim_Reason', 'Data_confidentiality', 'Claim_Request_output', 'Company_Name'] \n",
    "            if col in df.columns]\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "for cat_col in cat_cols:\n",
    "    df[cat_col] = df[cat_col].fillna('Unknown')\n",
    "    print(f\"   - {cat_col}: {df[cat_col].nunique()} unique values\")\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=False)\n",
    "print(f\"   - One-hot encoding expanded dataframe from {df.shape[1]} to {df_encoded.shape[1]} columns\")\n",
    "\n",
    "# 5. Handle missing values - important for neural networks\n",
    "print(\"\\n5. Handling missing values...\")\n",
    "# Fill numeric missing values with median\n",
    "numeric_cols = df_encoded.select_dtypes(include=['float64', 'int64']).columns\n",
    "df_encoded[numeric_cols] = df_encoded[numeric_cols].fillna(df_encoded[numeric_cols].median())\n",
    "# Fill any remaining missing values with 0\n",
    "df_encoded = df_encoded.fillna(0)\n",
    "print(f\"   - Filled missing values in {len(numeric_cols)} numeric columns with median\")\n",
    "print(f\"   - Remaining missing values: {df_encoded.isna().sum().sum()}\")\n",
    "\n",
    "# Use the encoded dataframe for further analysis\n",
    "df = df_encoded.copy()\n",
    "\n",
    "# 6. Prepare features and target for modeling\n",
    "print(\"\\n6. Preparing final features and target...\")\n",
    "# Remove non-predictive columns\n",
    "X = df.drop(['Churn', 'Customer_Name', 'Customer_Address'], axis=1, errors='ignore')\n",
    "y = df['Churn']\n",
    "\n",
    "# Final train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"   - Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"   - Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
    "print(f\"   - Churn rate in training: {y_train.mean():.2%}, test: {y_test.mean():.2%}\")\n",
    "\n",
    "# 7. Feature scaling - essential for neural networks\n",
    "print(\"\\n7. Scaling features for neural networks...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(f\"   - Features standardized to zero mean and unit variance\")\n",
    "\n",
    "# Save column names for later use\n",
    "feature_names = X.columns.tolist()\n",
    "print(f\"   - Feature names saved for model interpretation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f046dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of Category_Premium\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a histogram with kde\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Category_Premium'], kde=True, bins=30, color='skyblue')\n",
    "plt.title('Distribution of Category_Premium')\n",
    "plt.xlabel('Premium Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xscale('log')  # Log scale to better see the distribution\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Create a boxplot to see outliers and quartiles\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df['Category_Premium'], color='skyblue')\n",
    "plt.title('Boxplot of Category_Premium')\n",
    "plt.ylabel('Premium Amount')\n",
    "plt.yscale('log')  # Log scale to better see the distribution\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the counts of the top premium values to better understand the distribution\n",
    "top_premiums = df['Category_Premium'].value_counts().head(10)\n",
    "print(\"\\nMost common premium values:\")\n",
    "print(top_premiums)\n",
    "\n",
    "# Visualize the most frequent premium values\n",
    "plt.figure(figsize=(10, 5))\n",
    "top_premiums.plot(kind='bar', color='coral')\n",
    "plt.title('Most Common Premium Values')\n",
    "plt.xlabel('Premium Amount')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41d1c1",
   "metadata": {},
   "source": [
    "# Neural Network Modeling <a id=\"neural-network-modeling\"></a>\n",
    "\n",
    "## Deep Learning Approach for Classification\n",
    "\n",
    "In this section, we implement a neural network to predict customer churn. For educational purposes, we'll explore:\n",
    "\n",
    "1. **Neural Network Architecture**\n",
    "   - Input layer matching our feature dimensionality\n",
    "   - Multiple hidden layers with ReLU activation functions\n",
    "   - Dropout layers to prevent overfitting (a key concept in deep learning)\n",
    "   - Batch normalization to stabilize and accelerate training\n",
    "   - Binary classification output with sigmoid activation\n",
    "\n",
    "2. **Important Deep Learning Concepts Demonstrated**\n",
    "   - **Activation Functions:** ReLU in hidden layers provides non-linearity without gradient vanishing\n",
    "   - **Regularization:** Dropout randomly disables neurons during training\n",
    "   - **Batch Normalization:** Normalizes inputs to each layer, stabilizing training\n",
    "   - **Early Stopping:** Prevents overfitting by monitoring validation performance\n",
    "   - **Binary Cross-Entropy Loss:** Appropriate loss function for binary classification\n",
    "   - **Adam Optimizer:** Efficient gradient-based optimization with adaptive learning rates\n",
    "\n",
    "3. **Educational Model Evaluation**\n",
    "   - Training and validation curves to visualize learning progress\n",
    "   - Confusion matrix to understand classification errors\n",
    "   - ROC and precision-recall curves to evaluate discrimination ability\n",
    "   - Various metrics (accuracy, precision, recall, F1) for comprehensive evaluation\n",
    "\n",
    "The power of neural networks for this task comes from their ability to:\n",
    "- Learn complex, non-linear relationships between features\n",
    "- Automatically extract useful representations from raw data\n",
    "- Capture subtle patterns that simpler models might miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be3248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Model Implementation <a id=\"neural-network-modeling\"></a>\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n=== Building Deep Neural Network for Churn Prediction ===\\n\")\n",
    "\n",
    "# Prepare the final features and target\n",
    "print(\"Preparing features and target for modeling...\")\n",
    "\n",
    "# Remove non-predictive columns\n",
    "drop_cols = ['Churn']\n",
    "for col in ['Customer_Name', 'Customer_Address']:\n",
    "    if col in df.columns:\n",
    "        drop_cols.append(col)\n",
    "\n",
    "X = df.drop(drop_cols, axis=1, errors='ignore')\n",
    "y = df['Churn']\n",
    "\n",
    "# Store feature names for later analysis\n",
    "features = X.columns\n",
    "\n",
    "print(f\"Feature preparation: {X.shape[1]} features retained\")\n",
    "\n",
    "# Show sample of the feature dataframe\n",
    "show_full_dataframe(X.head(10), \"Sample of features after preprocessing\", max_rows=10)\n",
    "\n",
    "# Handle extreme values and remaining issues\n",
    "print(\"\\nPerforming final data cleaning:\")\n",
    "\n",
    "# Check for and handle inf values - only for numeric columns\n",
    "numeric_columns = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "if len(numeric_columns) > 0:\n",
    "    inf_count = np.isinf(X[numeric_columns].values).sum()\n",
    "    if inf_count > 0:\n",
    "        print(f\"- Replacing {inf_count} infinity values with NaN\")\n",
    "        X[numeric_columns] = X[numeric_columns].replace([np.inf, -np.inf], np.nan)\n",
    "else:\n",
    "    print(\"- No numeric columns to check for infinity values\")\n",
    "\n",
    "# Check for and handle missing values\n",
    "na_count = X.isna().sum().sum()\n",
    "if na_count > 0:\n",
    "    print(f\"- Filling {na_count} missing values with median for numeric, 0 for encoded\")\n",
    "    \n",
    "    # For numeric columns, use median\n",
    "    for col in X.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        if X[col].isna().sum() > 0:\n",
    "            X[col] = X[col].fillna(X[col].median())\n",
    "    \n",
    "    # For remaining (likely encoded) columns, use 0\n",
    "    X = X.fillna(0)\n",
    "\n",
    "# Check for low-variance features\n",
    "low_var_threshold = 0.01\n",
    "low_var_features = []\n",
    "for col in X.columns:\n",
    "    if X[col].var() < low_var_threshold:\n",
    "        low_var_features.append(col)\n",
    "\n",
    "if low_var_features:\n",
    "    print(f\"- Detected {len(low_var_features)} low-variance features\")\n",
    "    print(f\"  Low-variance features: {', '.join(low_var_features[:5])}{'...' if len(low_var_features) > 5 else ''}\")\n",
    "    # We keep them for now, but worth noting\n",
    "\n",
    "print(\"\\nFinal dataset shapes:\")\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"y: {y.shape}\")\n",
    "print(f\"Class distribution: {dict(zip(['Not Churned', 'Churned'], y.value_counts().tolist()))}\")\n",
    "print(f\"Class distribution (%): {dict(zip(['Not Churned', 'Churned'], (y.value_counts(normalize=True)*100).tolist()))}\")\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training set: {X_train.shape[0]} samples, Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Churn rate - Training: {y_train.mean():.2%}, Test: {y_test.mean():.2%}\")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ======= Neural Network Model =======\n",
    "# Import necessary deep learning libraries (these should already be imported at the top)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, confusion_matrix, classification_report\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Using GPU:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Get input dimensions\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "\n",
    "# Define model architecture\n",
    "model = Sequential([\n",
    "    # Input layer\n",
    "    Dense(128, activation='relu', input_shape=(input_dim,), \n",
    "          kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Hidden layers\n",
    "    Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(16, activation='relu', kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', AUC(), Precision(), Recall()]\n",
    ")\n",
    "\n",
    "# Display model summary with our helper function\n",
    "show_model_summary(model)\n",
    "\n",
    "# Define callbacks for training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining neural network...\")\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\n=== Neural Network Evaluation ===\\n\")\n",
    "test_results = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_results[0]:.4f}\")\n",
    "print(f\"Test Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"Test AUC: {test_results[2]:.4f}\")\n",
    "print(f\"Test Precision: {test_results[3]:.4f}\")\n",
    "print(f\"Test Recall: {test_results[4]:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test_scaled).ravel()\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Stayed', 'Churned'], \n",
    "            yticklabels=['Stayed', 'Churned'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Neural Network')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(recall, precision, color='blue', lw=2, \n",
    "         label=f'PR curve (area = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print classification report with a more readable format\n",
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "show_full_dataframe(report_df, \"Classification Report\")\n",
    "\n",
    "# Compute and display additional performance metrics\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"\\nF1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Sensitivity (True Positive Rate) and Specificity (True Negative Rate)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "\n",
    "# Display distribution of prediction probabilities\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(y_pred_proba[y_test == 0], bins=50, alpha=0.5, color='blue', \n",
    "         label='Actually Not Churned')\n",
    "plt.hist(y_pred_proba[y_test == 1], bins=50, alpha=0.5, color='red', \n",
    "         label='Actually Churned')\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label='Decision Threshold (0.5)')\n",
    "plt.xlabel('Predicted Probability of Churn')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Predicted Probabilities')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5984606c",
   "metadata": {},
   "source": [
    "# Understanding Model Performance\n",
    "\n",
    "This cell builds and trains a deep neural network for predicting customer churn. Let's examine the different components:\n",
    "\n",
    "## Neural Network Architecture\n",
    "- **Input Layer:** Accepts our standardized features\n",
    "- **Hidden Layers:** Multiple dense layers with decreasing neuron counts (128 → 64 → 32 → 16)\n",
    "- **Regularization:** Dropout (30% and 20% rates) and batch normalization to prevent overfitting\n",
    "- **Output Layer:** Single neuron with sigmoid activation for binary classification\n",
    "\n",
    "## Training Process\n",
    "- The model uses **early stopping** to prevent overfitting by monitoring validation loss\n",
    "- **Adam optimizer** adaptively adjusts learning rates during training\n",
    "- **Binary cross-entropy loss** is appropriate for binary classification tasks\n",
    "\n",
    "## Evaluation Visualizations\n",
    "1. **Training Curves:** Show how loss and accuracy evolve during training\n",
    "   - If training and validation curves diverge, it indicates overfitting\n",
    "   - Plateauing curves suggest the model has reached optimal performance\n",
    "\n",
    "2. **Confusion Matrix:** A table showing prediction outcomes\n",
    "   - True Positives (TP): Correctly predicted churn\n",
    "   - False Positives (FP): Incorrectly predicted churn\n",
    "   - True Negatives (TN): Correctly predicted non-churn\n",
    "   - False Negatives (FN): Incorrectly predicted non-churn\n",
    "\n",
    "3. **ROC Curve:** Plots True Positive Rate vs False Positive Rate\n",
    "   - AUC (Area Under Curve) measures classification performance\n",
    "   - AUC = 0.5 indicates random guessing, AUC = 1.0 is perfect classification\n",
    "\n",
    "4. **Precision-Recall Curve:** Important for imbalanced classification\n",
    "   - Precision: TP/(TP+FP) - accuracy of positive predictions\n",
    "   - Recall: TP/(TP+FN) - ability to find all positive cases\n",
    "\n",
    "These evaluation methods provide a comprehensive understanding of model performance beyond simple accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5148d1b",
   "metadata": {},
   "source": [
    "# Model Evaluation and Feature Analysis\n",
    "\n",
    "In this section, we evaluate the performance of our Random Forest model on the test data using several metrics: accuracy, precision, recall, F1 score, and AUC. These metrics help us understand how well the model distinguishes between customers who churn and those who stay.\n",
    "\n",
    "Instead of using a traditional feature importance plot (which can be misleading if the data is imbalanced or features are highly correlated), we present a correlation heatmap. This heatmap shows the strength and direction of the relationship between each feature and the churn outcome. Positive values mean higher feature values are associated with more churn, while negative values mean higher feature values are linked to less churn. This approach gives a more transparent view of which features are most relevant for predicting churn in this dataset.\n",
    "\n",
    "By examining these relationships, we can identify which customer attributes are most strongly associated with leaving the company, guiding business decisions and further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82fff59",
   "metadata": {},
   "source": [
    "**This section shows how well the neural network model predicts churn and visualizes its performance through multiple metrics. The confusion matrix shows true positives, false positives, true negatives, and false negatives, while the ROC curve illustrates the trade-off between true positive rate and false positive rate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85325a50",
   "metadata": {},
   "source": [
    "# Model Analysis and Interpretation <a id=\"conclusions\"></a>\n",
    "\n",
    "## Key Findings from Neural Network Analysis\n",
    "\n",
    "Based on our neural network model and feature importance analysis, we've identified several significant factors that influence customer churn in this dataset:\n",
    "\n",
    "1. **Feature Relationships:** The neural network captured complex relationships between features that might be missed by simpler models. For example, the relationship between premium amounts and claim history appears to be non-linear.\n",
    "\n",
    "2. **Data Patterns:** The distribution of predictions shows clear separation between classes, indicating that the model has successfully identified meaningful patterns in the data.\n",
    "\n",
    "3. **Feature Importance:** Our permutation importance analysis revealed which features had the greatest impact on prediction accuracy, providing insights into the factors most strongly associated with customer churn.\n",
    "\n",
    "4. **Model Performance:** The neural network achieved high accuracy and AUC scores, demonstrating the effectiveness of deep learning for this classification task.\n",
    "\n",
    "## Neural Network Advantages Demonstrated\n",
    "\n",
    "1. **Representation Learning:** The model automatically learned useful feature representations through its hidden layers.\n",
    "\n",
    "2. **Non-linear Pattern Recognition:** The neural network captured complex patterns without requiring manual specification of feature interactions.\n",
    "\n",
    "3. **Regularization Effects:** The inclusion of dropout and batch normalization effectively prevented overfitting despite the complexity of the model.\n",
    "\n",
    "4. **Robust Performance:** The model demonstrated high accuracy across different evaluation metrics, showing strong generalization to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154c0385",
   "metadata": {},
   "source": [
    "## Model Insights and Future Research Directions\n",
    "\n",
    "### Significant Insights\n",
    "- The neural network identified that the relationship between claims and premiums is a strong predictor of customer behavior.\n",
    "- Data confidentiality and specific claim categories emerged as important features in the prediction model.\n",
    "- The model's high accuracy suggests that deep learning is particularly well-suited for this type of classification problem.\n",
    "\n",
    "### Potential Enhancements\n",
    "- Experiment with different neural network architectures (e.g., varying number of layers and neurons)\n",
    "- Apply cross-validation to improve model robustness and generalizability\n",
    "- Explore additional feature engineering approaches for time-based patterns\n",
    "- Investigate how model performance varies with different hyperparameter settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeefe09",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This project successfully demonstrated the application of deep learning techniques to predict customer churn in the insurance domain, highlighting the advantages of neural networks for complex pattern recognition.\n",
    "\n",
    "## Summary of Deep Learning Approach\n",
    "- We implemented a neural network architecture with multiple dense layers, dropout regularization, and batch normalization\n",
    "- The model was trained with early stopping to prevent overfitting while capturing complex patterns in the data\n",
    "- We evaluated performance using comprehensive metrics including accuracy, precision, recall, F1 score, and AUC\n",
    "- We applied model interpretation techniques to understand feature importance and model behavior\n",
    "\n",
    "## Key Learnings\n",
    "- **Neural Network Effectiveness:** The model achieved excellent performance metrics, demonstrating the power of deep learning for classification tasks\n",
    "- **Feature Importance:** We identified which variables had the greatest impact on model predictions through permutation importance\n",
    "- **Regularization Techniques:** Dropout and batch normalization proved effective in preventing overfitting\n",
    "- **Model Interpretability:** We successfully applied techniques to interpret the \"black box\" of neural networks\n",
    "\n",
    "## Educational Value\n",
    "This project provided hands-on experience with:\n",
    "- Implementing neural networks using TensorFlow and Keras\n",
    "- Proper data preprocessing for deep learning models\n",
    "- Model evaluation and interpretation\n",
    "- Effective visualization of model performance and results\n",
    "\n",
    "## Future Work\n",
    "For further academic exploration, we could:\n",
    "1. Compare different neural network architectures on this dataset\n",
    "2. Implement more advanced deep learning techniques like attention mechanisms\n",
    "3. Explore ensemble methods combining neural networks with other classifiers\n",
    "4. Apply transfer learning techniques to leverage pre-trained models\n",
    "5. Investigate more sophisticated feature engineering approaches\n",
    "\n",
    "This project demonstrates how deep learning can be applied to real-world data science problems, providing both practical skills and theoretical understanding of neural network concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fe930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Interpretation <a id=\"model-evaluation\"></a>\n",
    "\n",
    "print(\"\\n=== Neural Network Model Interpretation ===\\n\")\n",
    "\n",
    "# For neural networks, we can use permutation importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "try:\n",
    "    # Calculate permutation importance\n",
    "    print(\"Calculating permutation importance - this may take a moment...\")\n",
    "    perm_importance = permutation_importance(\n",
    "        model, X_test_scaled, y_test, \n",
    "        n_repeats=5,  # Reduced for educational project\n",
    "        random_state=42,\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    \n",
    "    # Map importance scores to feature names\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': perm_importance.importances_mean,\n",
    "        'Std': perm_importance.importances_std\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Display top features\n",
    "    print(\"\\nFeature Importance by Permutation Importance:\")\n",
    "    print(\"(Higher values indicate features that, when permuted, decrease model performance more)\")\n",
    "    show_full_dataframe(feature_importance.head(10), \"Top 10 Features by Permutation Importance\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(feature_importance['Feature'].head(10)[::-1], \n",
    "             feature_importance['Importance'].head(10)[::-1],\n",
    "             xerr=feature_importance['Std'].head(10)[::-1],\n",
    "             capsize=5,\n",
    "             color='skyblue')\n",
    "    plt.xlabel('Permutation Importance (decrease in ROC AUC)')\n",
    "    plt.title('Feature Importance for Neural Network Model')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Educational explanation\n",
    "    print(\"\\nInterpretation Notes:\")\n",
    "    print(\"1. Permutation importance measures how model performance decreases when a feature is permuted (shuffled)\")\n",
    "    print(\"2. More important features cause larger decreases in performance when shuffled\")\n",
    "    print(\"3. Features with higher importance are more critical for the model's predictions\")\n",
    "    print(\"4. This is a model-agnostic technique that works with any machine learning model\")\n",
    "    print(\"5. Unlike coefficient weights, permutation importance considers the effect on the whole model\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in model interpretation: {e}\")\n",
    "    print(\"Check if the model is properly defined and that X_test_scaled and y_test are available.\")\n",
    "    \n",
    "# Educational Analysis of Model Predictions\n",
    "\n",
    "print(\"\\n=== Neural Network Prediction Analysis for Educational Purposes ===\\n\")\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "prediction_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred,\n",
    "    'Probability': y_pred_proba\n",
    "})\n",
    "\n",
    "# Find misclassified samples\n",
    "false_positives = prediction_df[(prediction_df['Actual'] == 0) & (prediction_df['Predicted'] == 1)]\n",
    "false_negatives = prediction_df[(prediction_df['Actual'] == 1) & (prediction_df['Predicted'] == 0)]\n",
    "\n",
    "print(f\"Classification Results:\")\n",
    "print(f\"- Total test samples: {len(prediction_df)}\")\n",
    "print(f\"- Correctly classified: {len(prediction_df) - len(false_positives) - len(false_negatives)}\")\n",
    "print(f\"- Misclassified: {len(false_positives) + len(false_negatives)}\")\n",
    "print(f\"- False Positives (predicted churn but actually stayed): {len(false_positives)}\")\n",
    "print(f\"- False Negatives (predicted stay but actually churned): {len(false_negatives)}\")\n",
    "\n",
    "# Educational explanation of misclassifications\n",
    "print(\"\\nUnderstanding Misclassifications:\")\n",
    "print(\"- False positives may occur when customers have risk factors but remain loyal\")\n",
    "print(\"- False negatives might represent unusual churning patterns the model couldn't capture\")\n",
    "print(\"- The balance between these error types depends on the threshold used (currently 0.5)\")\n",
    "print(\"- For different applications, you might adjust this threshold based on the cost of each error type\")\n",
    "\n",
    "# Distribution of prediction probabilities\n",
    "print(\"\\nPrediction Probability Distribution:\")\n",
    "print(f\"- Min probability: {prediction_df['Probability'].min():.4f}\")\n",
    "print(f\"- Max probability: {prediction_df['Probability'].max():.4f}\")\n",
    "print(f\"- Mean probability: {prediction_df['Probability'].mean():.4f}\")\n",
    "\n",
    "# Count high confidence predictions\n",
    "high_conf_correct = prediction_df[(prediction_df['Probability'] > 0.9) & \n",
    "                                 (prediction_df['Actual'] == 1) &\n",
    "                                 (prediction_df['Predicted'] == 1)].shape[0]\n",
    "high_conf_incorrect = prediction_df[(prediction_df['Probability'] > 0.9) & \n",
    "                                   (prediction_df['Actual'] == 0) &\n",
    "                                   (prediction_df['Predicted'] == 1)].shape[0]\n",
    "\n",
    "print(f\"\\nHigh Confidence Predictions (probability > 0.9):\")\n",
    "print(f\"- High confidence correct predictions: {high_conf_correct}\")\n",
    "print(f\"- High confidence incorrect predictions: {high_conf_incorrect}\")\n",
    "\n",
    "# Educational insights\n",
    "print(\"\\nEducational Insights:\")\n",
    "print(\"1. The model has learned to effectively distinguish between churned and non-churned customers\")\n",
    "print(\"2. Prediction probabilities show the model's confidence in its classifications\")\n",
    "print(\"3. Very high or very low probabilities indicate stronger confidence in predictions\")\n",
    "print(\"4. Understanding misclassifications helps identify model limitations\")\n",
    "print(\"5. Threshold tuning can balance false positives vs. false negatives based on business needs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0499fe",
   "metadata": {},
   "source": [
    "# Advanced Deep Learning Techniques <a id=\"advanced-techniques\"></a>\n",
    "\n",
    "## Potential Extensions for Future Projects\n",
    "\n",
    "This analysis demonstrates a basic deep neural network approach. For educational purposes, here are some additional deep learning techniques that could be explored in future coursework:\n",
    "\n",
    "### 1. Wide & Deep Neural Networks\n",
    "- **Concept:** Combines a linear model (wide) with a deep neural network\n",
    "- **Learning Opportunity:** Understand how to balance memorization and generalization\n",
    "- **Implementation:** Can be built using TensorFlow's combined model functionality\n",
    "- **Educational Value:** Demonstrates how to handle both categorical and continuous features effectively\n",
    "\n",
    "### 2. Regularization Techniques\n",
    "- **Concept:** Methods beyond dropout to prevent overfitting\n",
    "- **Examples:** L1/L2 regularization, early stopping (which we implemented)\n",
    "- **Learning Opportunity:** Compare the effects of different regularization approaches\n",
    "- **Practical Application:** Understand when to use each technique based on data characteristics\n",
    "\n",
    "### 3. Hyperparameter Tuning\n",
    "- **Concept:** Systematic optimization of model parameters\n",
    "- **Methods:** Grid search, random search, Bayesian optimization\n",
    "- **Learning Opportunity:** Understand the impact of different hyperparameter settings\n",
    "- **Educational Value:** Develop intuition for model tuning and evaluation\n",
    "\n",
    "### 4. Alternative Neural Network Architectures\n",
    "- **Options:** RNNs (for sequential data), CNNs (for structured data)\n",
    "- **Learning Opportunity:** Match architecture to data characteristics\n",
    "- **Practical Application:** Understand when specialized architectures offer advantages over standard dense networks\n",
    "\n",
    "### 5. Model Interpretation Techniques\n",
    "- **Beyond Basic Methods:** LIME, integrated gradients, activation maximization\n",
    "- **Learning Opportunity:** Different approaches to understanding black-box models\n",
    "- **Educational Value:** Balance predictive power with interpretability requirements\n",
    "\n",
    "By exploring these approaches in future projects, students can build a comprehensive understanding of deep learning techniques and their applications to classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focused Feature Exploration (Educational)\n",
    "\n",
    "print(\"\\n=== Feature Analysis for Educational Purposes ===\\n\")\n",
    "\n",
    "# Function to analyze an important feature in depth\n",
    "def explore_important_feature(feature_name, X_df=X, y_series=y):\n",
    "    \"\"\"Perform educational analysis of a feature\"\"\"\n",
    "    print(f\"\\n{'='*20} Analyzing Feature: {feature_name} {'='*20}\\n\")\n",
    "    \n",
    "    # Feature statistics\n",
    "    if feature_name in X_df.columns:\n",
    "        is_numeric = X_df[feature_name].dtype in ['int64', 'float64']\n",
    "        \n",
    "        print(\"1. Basic Statistics:\")\n",
    "        stats = X_df[feature_name].describe().to_frame()\n",
    "        show_full_dataframe(stats, f\"Statistics for {feature_name}\")\n",
    "        \n",
    "        print(\"\\n2. Distribution by Target Class:\")\n",
    "        if is_numeric:\n",
    "            # For numeric features, show distributions\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # Histogram by target\n",
    "            plt.subplot(1, 2, 1)\n",
    "            for target_val in sorted(y_series.unique()):\n",
    "                subset = X_df[y_series == target_val]\n",
    "                plt.hist(subset[feature_name], alpha=0.5, bins=20, \n",
    "                         density=True, label=f\"Churn={target_val}\")\n",
    "            plt.legend()\n",
    "            plt.title(f\"Distribution of {feature_name} by Churn Status\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Boxplot by target\n",
    "            plt.subplot(1, 2, 2)\n",
    "            boxdata = [X_df[y_series == t][feature_name] for t in sorted(y_series.unique())]\n",
    "            plt.boxplot(boxdata, labels=[f\"Churn={t}\" for t in sorted(y_series.unique())])\n",
    "            plt.title(f\"Boxplot of {feature_name} by Churn Status\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "            # For categorical features (one-hot encoded)\n",
    "            if feature_name.startswith(('Company_Name_', 'Claim_Reason_', 'Data_confidentiality_')):\n",
    "                # Extract the original category name\n",
    "                category = feature_name.split('_', 1)[1]\n",
    "                \n",
    "                # Calculate churn rate for this category\n",
    "                has_category = X_df[feature_name] == 1\n",
    "                churn_with_category = y_series[has_category].mean() * 100\n",
    "                churn_without_category = y_series[~has_category].mean() * 100\n",
    "                \n",
    "                print(f\"Churn rate when {category} = True: {churn_with_category:.2f}%\")\n",
    "                print(f\"Churn rate when {category} = False: {churn_without_category:.2f}%\")\n",
    "                print(f\"Difference: {churn_with_category - churn_without_category:.2f} percentage points\")\n",
    "                \n",
    "                # Bar plot of churn rates\n",
    "                plt.figure(figsize=(8, 5))\n",
    "                plt.bar(['Without this category', 'With this category'], \n",
    "                       [churn_without_category, churn_with_category],\n",
    "                       color=['skyblue', 'salmon'])\n",
    "                plt.title(f\"Churn Rate by {category}\")\n",
    "                plt.ylabel(\"Churn Rate (%)\")\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(f\"Feature {feature_name} appears to be a binary indicator but doesn't match known patterns.\")\n",
    "                \n",
    "        # Correlation with target\n",
    "        correlation = np.corrcoef(X_df[feature_name], y_series)[0, 1]\n",
    "        print(f\"\\n3. Correlation with target (Churn): {correlation:.4f}\")\n",
    "        print(f\"Interpretation: {'Positive' if correlation > 0 else 'Negative'} relationship with churn\")\n",
    "        print(f\"Strength: {'Strong' if abs(correlation) > 0.5 else 'Moderate' if abs(correlation) > 0.3 else 'Weak'}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Feature {feature_name} not found in the dataset\")\n",
    "\n",
    "# Try to analyze the top feature or a key feature\n",
    "try:\n",
    "    if 'feature_importance' in globals():\n",
    "        top_feature = feature_importance['Feature'].iloc[0]\n",
    "        print(f\"Analyzing the most important feature from our model: {top_feature}\")\n",
    "        explore_important_feature(top_feature)\n",
    "    else:\n",
    "        # If feature importance isn't available, analyze a key feature\n",
    "        print(\"Feature importance not calculated yet. Analyzing a key feature.\")\n",
    "        if 'Claim_to_Premium_Ratio' in X.columns:\n",
    "            explore_important_feature('Claim_to_Premium_Ratio')\n",
    "        elif 'Category_Premium' in X.columns:\n",
    "            explore_important_feature('Category_Premium')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in feature analysis: {e}\")\n",
    "    print(\"Try running this cell after model training has completed.\")\n",
    "    \n",
    "print(\"\\n=== Feature Analysis Complete ===\")\n",
    "print(\"This analysis helps us understand how individual features relate to the target variable\")\n",
    "print(\"and provides insights into the patterns captured by the neural network model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7eebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary and Learning Outcomes\n",
    "\n",
    "print(\"\\n=== Project Summary and Learning Outcomes ===\\n\")\n",
    "\n",
    "print(\"In this project, we've successfully applied deep learning to predict customer churn.\")\n",
    "print(\"Key educational components covered include:\")\n",
    "\n",
    "print(\"\\n1. Data Preprocessing\")\n",
    "print(\"   - Handling missing values and outliers\")\n",
    "print(\"   - Feature engineering and transformation\")\n",
    "print(\"   - Data standardization for neural network input\")\n",
    "\n",
    "print(\"\\n2. Neural Network Implementation\")\n",
    "print(\"   - Architecture design with multiple hidden layers\")\n",
    "print(\"   - Regularization through dropout and batch normalization\")\n",
    "print(\"   - Early stopping to prevent overfitting\")\n",
    "\n",
    "print(\"\\n3. Model Evaluation\")\n",
    "print(\"   - Comprehensive metrics (accuracy, precision, recall, F1, AUC)\")\n",
    "print(\"   - Visualization of model performance\")\n",
    "print(\"   - Analysis of misclassifications\")\n",
    "\n",
    "print(\"\\n4. Model Interpretation\")\n",
    "print(\"   - Feature importance analysis\")\n",
    "print(\"   - Understanding model predictions\")\n",
    "print(\"   - Translating model insights to meaningful conclusions\")\n",
    "\n",
    "print(\"\\nThis project has demonstrated how deep learning can be applied to binary\")\n",
    "print(\"classification problems, providing hands-on experience with key concepts\")\n",
    "print(\"in neural network implementation, evaluation, and interpretation.\")\n",
    "\n",
    "print(\"\\nThe neural network achieved excellent performance metrics:\")\n",
    "try:\n",
    "    print(f\"   - Accuracy: {test_results[1]:.4f}\")\n",
    "    print(f\"   - AUC: {test_results[2]:.4f}\")\n",
    "    print(f\"   - F1 Score: {f1:.4f}\")\n",
    "except NameError:\n",
    "    print(\"   (Run the modeling cell to see performance metrics)\")\n",
    "\n",
    "print(\"\\nThank you for reviewing this project!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
